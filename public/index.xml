<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Sandbox</title>
    <link>/</link>
    <description>Recent content on Sandbox</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Wed, 04 Mar 2020 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Markov Chain Monte Carlo (MCMC) - Differential Evolution</title>
      <link>/2020/03/04/markov-chain-monte-carlo-mcmc-differential-evolution/</link>
      <pubDate>Wed, 04 Mar 2020 00:00:00 +0000</pubDate>
      
      <guid>/2020/03/04/markov-chain-monte-carlo-mcmc-differential-evolution/</guid>
      <description>Based on “A simple introduction to Markov Chain Monte–Carlo sampling” by Ravenzwaaij et al., and blog post by Pablo R. Mier
Introduction In two previous blog posts, we covered the basics of MCMC and 2 common implementations - Metropolis-Hastings Algorithm1, and Gibbs Sampling. In this post, I will discuss the last algorithm discussed in the Ravenzwaaij paper cited above - Differential Evolution (DE).
 Motivation for DE? As discussed previously, the sampling procedure used by the Metropolis/Gibbs sampling algorithms may sometimes be a hindrance to fast convergence (e.</description>
    </item>
    
    <item>
      <title>Markov Chain Monte Carlo (MCMC) - Gibbs Sampling</title>
      <link>/2020/03/03/markov-chain-monte-carlo-mcmc-gibbs-sampling/</link>
      <pubDate>Tue, 03 Mar 2020 00:00:00 +0000</pubDate>
      
      <guid>/2020/03/03/markov-chain-monte-carlo-mcmc-gibbs-sampling/</guid>
      <description>Based on “A simple introduction to Markov Chain Monte–Carlo sampling” by Ravenzwaaij et al.:
Introduction In an earlier post, I discussed the basic idea of MCMC methods, and provided an overview of the Metropolis algorithm. In this section, I’ll jump straight into the motivation for Gibbs Sampling, and an overview of how the algorithm works.
 What is the point of Gibbs Sampling if we already know the Metropolis algorithm?</description>
    </item>
    
    <item>
      <title>Markov Chain Monte Carlo (MCMC) - Metropolis Hastings</title>
      <link>/2020/03/01/markov-chain-monte-carlo-mcmc-metropolis-hastings/</link>
      <pubDate>Sun, 01 Mar 2020 00:00:00 +0000</pubDate>
      
      <guid>/2020/03/01/markov-chain-monte-carlo-mcmc-metropolis-hastings/</guid>
      <description>Based on “A simple introduction to Markov Chain Monte–Carlo sampling” by Ravenzwaaij et al.:
I’ve been meaning to do a proper introduction to MCMC piece for a while now, but a crippling ignorance of how MCMC actually works has thus far stymied me. So this paper by Ravenwaaij et al. has pretty much been a god-send. I’m building up towards a piece experimenting with Kay Brodersen’s CausalImpact package, so will be an intermediate signpost on that journey.</description>
    </item>
    
    <item>
      <title>[Adapted] Best Pokemon Team</title>
      <link>/2019/12/16/reproduced-best-pokemon-team/</link>
      <pubDate>Mon, 16 Dec 2019 00:00:00 +0000</pubDate>
      
      <guid>/2019/12/16/reproduced-best-pokemon-team/</guid>
      <description>pre { font-size: 15px } p.caption { font-size: 13px; color: grey; font-style: italic; text-align: center; }  Adapted from Emily Robinson. Data from robinsones. Introduction Came across this fairly interesting blog post recently. Given that there are 18 types of pokemon, some of which are super-effective/not very effective against each other, how do we get a pokemon team combination that is super effective against most other teams?
## Attacking Normal Fire Water Electric Grass Ice Fighting Poison Ground Flying ## 1 Normal 1 1.</description>
    </item>
    
    <item>
      <title>[Reproduced] Spurious Correlations and Random Walks</title>
      <link>/2019/12/12/spurious-correlations-and-random-walks/</link>
      <pubDate>Thu, 12 Dec 2019 00:00:00 +0000</pubDate>
      
      <guid>/2019/12/12/spurious-correlations-and-random-walks/</guid>
      <description>pre { font-size: 15px } p.caption { font-size: 13px; color: grey; font-style: italic; text-align: center; }  Reproduced from Fabian Dablander Introduction This short post will explore the implications of random walk processes in inducing spurious correlations and false positives in hypothesis testing.
Recall that an AR series generally takes the following form:
 \(Y_{t} = \phi Y_{t-1} + \epsilon_{t}\) — (1)  
where \(\phi\) measures the autocorrelation, and \(\epsilon \sim \mathbb{N}(0,\sigma^{2})\).</description>
    </item>
    
    <item>
      <title>About</title>
      <link>/about/</link>
      <pubDate>Mon, 25 Nov 2019 21:48:51 -0700</pubDate>
      
      <guid>/about/</guid>
      <description>Personal code playground. Written with blogdown.
Theme by @jrutheiser/hugo-lithium-theme and @yihui/hugo-lithium. Site built using walkthrough by Alison Hill and blogdown book by Xie Yihui</description>
    </item>
    
    <item>
      <title>How to Markdown - Formatting in Markdown</title>
      <link>/2019/11/25/how-to-markdown-formatting-in-markdown/</link>
      <pubDate>Mon, 25 Nov 2019 00:00:00 +0000</pubDate>
      
      <guid>/2019/11/25/how-to-markdown-formatting-in-markdown/</guid>
      <description>R Markdown Part 1: Formatting in Markdown This is the first post in what will hopefully transpire to be a multi-segment post, just to get familiar with Markdown syntax and options.
Posts will be written with .rmd because:
It allows the entire workflow to be done within RStudio, and
 The amazing addition of the reticulate:: library allows code to be writen in python and R seamlessly  Today’s post will deal with Pandoc’s markdown syntax.</description>
    </item>
    
    <item>
      <title>Hello Markdown</title>
      <link>/2016/12/30/hello-markdown/</link>
      <pubDate>Fri, 30 Dec 2016 00:00:00 +0000</pubDate>
      
      <guid>/2016/12/30/hello-markdown/</guid>
      <description>This is a post written in plain Markdown (*.md) instead of R Markdown (*.Rmd). The major differences are:
You cannot run any R code in a plain Markdown document, whereas in an R Markdown document, you can embed R code chunks (```{r}); A plain Markdown post is rendered through Blackfriday, and an R Markdown document is compiled by rmarkdown and Pandoc.  There are many differences in syntax between Blackfriday’s Markdown and Pandoc’s Markdown.</description>
    </item>
    
    <item>
      <title>Hello R Markdown</title>
      <link>/2015/07/23/hello-r-markdown/</link>
      <pubDate>Thu, 23 Jul 2015 21:13:14 -0500</pubDate>
      
      <guid>/2015/07/23/hello-r-markdown/</guid>
      <description>R Markdown This is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see http://rmarkdown.rstudio.com.
You can embed an R code chunk like this:
summary(cars) ## speed dist ## Min. : 4.0 Min. : 2.00 ## 1st Qu.:12.0 1st Qu.: 26.00 ## Median :15.0 Median : 36.00 ## Mean :15.4 Mean : 42.98 ## 3rd Qu.</description>
    </item>
    
    <item>
      <title>lorem ipsum</title>
      <link>/2015/01/01/lorem-ipsum/</link>
      <pubDate>Thu, 01 Jan 2015 00:00:00 +0000</pubDate>
      
      <guid>/2015/01/01/lorem-ipsum/</guid>
      <description>Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.</description>
    </item>
    
  </channel>
</rss>